---
title: 'Exploration 2: Description of Relationships'
author: 'Jake Bowers'
date: '`r format(Sys.Date(), "%B %d, %Y")`'
header-includes:
    - \usepackage[T1]{fontenc}
    - \usepackage{textcomp}
    - \usepackage{fontspec}
    - \newfontfamily\unicodefont[Ligatures=TeX]{TeX Gyre Heros}
    - \newfontfamily\grouptwofont[Ligatures=TeX]{Source Code Pro}
    - \newfontfamily\groupthreefont[Ligatures=TeX]{Courier}
output:
  pdf_document:
    number_sections: true
    fig_caption: yes
    fig_height: 8
    fig_width: 8
    latex_engine: xelatex
    citation_package: biblatex
    keep_tex: true
fontsize: 10pt
geometry: margin=1in
graphics: yes
mainfont: "Helvetica"
bibliography: classbib.bib
biblio-style: "authoryear-comp,natbib"
---

<!-- Make this document using library(rmarkdown); render("exploration2.Rmd") -->
\input{mytexsymbols}

```{r setup, echo=FALSE, results=FALSE, include=FALSE, cache=FALSE}
library(here)
source(here("rmd_setup.R"))
library(tidyverse)
```

"Thanks!" Your friend's voice sounds much calmer than it did when you last
spoke. "My tasks this week are much less scary than engaging with long-tailed
and zero-inflated variables like hours spent helping others and groups." When
you ask what she is up to, she replies,"I do need some help. At a recent
meeting we had a debate about two different statements. Can you help me by
providing answers based in data? We got into a shouting matching and only
stopped when I mentioned the word 'data'. I don't know much
about data. Can you help? Here are the two statements."

> Support for anti-immigration populists like Trump or the UKIP party arises from anti-feminism more than nativism or or socioeconomic characteristics like income or education."

> "I managed to find a survey that is supposed to be representative of people in the UK."

```{r readdata}
# library(foreign) ## For older Stata files
library(readstata13)
bes <- read.dta13("http://jakebowers.org/Data/BES2015/bes_f2f_original_v3.0.dta", convert.factors = FALSE)
```

"And I found a
(codebook)[http://www.jakebowers.org/Data/BES2015/BES_2015_f2f_v3.0_codebook.pdf]
too. And have learned enough about R to investigate and even recode some variables. I'm sure I didn't do enough to get rid of missing values, change values to make the variables more meaningful, etc."

```{r sometabs, results="hide"}
table(bes$y09, exclude = c()) ## Male=1, Female=2 gender (41 of 49 in footer)
bes$female <- as.numeric(bes$y09 == 2) ## make a variable called female=1 if female and 0 if not
## Check the recode
with(bes, table(female, y09, exclude = c()))
table(bes$r03, useNA = "ifany") ##  Support for equality for women (page 29 of 49), search for "R 3"
table(bes$r03, exclude = c()) ## lower numbers less support for equality for women.
## Give the variable an easier name to remeber and code don't know (DK) answers as
## missing. I wonder if the DK people oppose equality? Or why they say DK?
bes$women_equal <- with(bes, ifelse(r03 == -1, NA, r03))
## Check the recode
with(bes, table(women_equal, r03, exclude = c()))
table(bes$Age, useNA = "ifany")
table(bes$y01, useNA = "ifany") ## Income (37 of 49), higher=higher income bracket
## Recode negative numbers to be missing
bes$income <- with(bes, ifelse(y01<0,NA,y01))
table(bes$j05, useNA = "ifany") ## Immigration (15 of 49)
table(bes$education, useNA = "ifany") ## based on y 13a
## Create an education level variable
bes <- bes %>% mutate(edlevel = case_when(
  education == 12 ~ 1,
  education == 10 ~ 2,
  education == 11 ~ 2,
  education == 7 ~ 3,
  education == 8 ~ 3,
  education == 3 ~ 4,
  education == 2 ~ 4,
  education == 1 ~ 5,
  education == 0 ~ 0,
  education == 17 ~ 1,
  education == 15 ~ 1,
  education == 16 ~ 2,
  education == 14 ~ 2,
  education == 13 ~ 3,
  education == 5 ~ 4,
  education == 4 ~ 4,
  education == 18 ~ NA_real_,
  education == 6 ~ 3,
  education == 9 ~ 3
))
# edlevel 0 "No qualifications" 1 "GCSE D-G" 2 "GCSE A*-C" 3 "A-level" 4 "Undergraduate" 5 "Postgrad" 6"other"
with(bes, table(education, edlevel, exclude = c()))
## Convert education level into categories in case that is easier than treating
## it as a continuous numeric variable.
bes$edlevelF <- factor(bes$edlevel, labels = c(
  "No qualifications",
  "GCSE D-G", "GCSE A*-C", "A-level", "Undergraduate", "Postgrad"
))
with(bes, table(edlevel, edlevelF, exclude = c()))
table(bes$b02, useNA = "ifany") ## Party voted in last election (4 of 49)
table(bes$b04, useNA = "ifany") ## If you have voted which party would you have voted for
## 1 if voted or would have voted for ukip
bes$ukipvoter <- with(bes, as.numeric(ifelse(is.na(b02), b04 == 8, b02 == 7)))
table(bes$ukipvoter, useNA = "ifany")
table(bes$d01, useNA = "ifany") # Party ID (11 of 49)
## UKIP and British National Party are basically the same, I think
bes$ukip_pid <- as.numeric(bes$d01 %in% c(2, 8))
with(bes, table(d01, ukip_pid, exclude = c()))
```

> What do you think? Between these variables, and the others in the codebook,
can you provide some evidence for or against those statements? Since this was
such a heated discussion in our staff meeting, I'd like to see at least three
ways to describe these two-way relationships including that least squares stuff that
people always talk about. I'll need you to justify the weaknesses and
strengths of the tactics you choose to use. And, especially for the least
squares description, can you please help me understand why it is called least
squares and why I should care about least squares versus most squares or least
something else?  Right now, we just want to describe relationships so please
don't show me standard errors, $p$-values, confidence intervals, or posterior
density intervals (this also means that "homoskedasticity" and
"heteroskedasticity" and "statistical power" and "precision" and "efficiency"
and "normality" are also words I don't need to hear). I really just want to know
about **relationships** in the data at hand. Mostly these are represented by the
**coefficients** of different data models/line fitters/smoothers.

> I found R commands like `lm`, `loess`, `lmrob` (in the robustbase package),
> `rlm` (in the MASS package),
`rq` ( in the quantreg package) as ways to summarize two dimensional
relationships.

> I'm particularly confused about "least squares". A friend sent me some code
> that she claims "does least squares" but I'm not even sure what that means or
> why I should care. I also thought that you could find the least squares
> solution in one line rather than needing a whole function. 


```{r}
sum_sq_resids <- function(a, b, y, z) {
  yhat <- a + b * z
  ehat <- y - yhat
  thessr <- sum(ehat^2)
  return(thessr)
}

small_dat <- dplyr::select(bes,ukipvoter, income, women_equal, edlevel,
    edlevelF, j05) %>% na.omit()

## a has to be between 0 and 1, b also should be between 0 and 1.
## a=mean of ukipvoter when income=0 or proportion of ukipvoter when income=0
## b=difference in proportion of ukipvoter between income=0 and income=1. (or
## any 1 unit difference)

## Is this the least squares solution?
sum_sq_resids(a = .1, b = .12, y = small_dat$ukipvoter, z = small_dat$income)
## Or this?
sum_sq_resids(a = 0, b = .2, y = small_dat$ukipvoter, z = small_dat$income)

possible_solutions <- expand_grid(a=seq(0,1,.1),b=seq(-1,1,.1))

library(purrr)

res <- pmap_dbl(possible_solutions,.f=function(a,b){ sum_sq_resids(a =a, b = b, y = small_dat$ukipvoter, z = small_dat$income) })

possible_solutions$ssr <- res

## The lowest sum of squared residuals that we tried:
## So does this mean that the difference in ukip voting for each unit of the
## income scale is 0?
filter(possible_solutions, ssr==min(ssr))

## Hmmm... maybe there is a better solution. Is a=.1 and b=0 really the
## **least** one?

## Lets ask R to find the lowest ssr.
sum_sq_resids_vec <- function(x, y, z) {
    # Allowing the coefficients to be a vector
  yhat <- x[1] + x[2] * z
  ehat <- y - yhat
  thessr <- sum(ehat^2)
  return(thessr)
}


## Seems like there is a better solution that we missed.
res2 <- optim(par=c(0,0),fn=sum_sq_resids_vec,y = small_dat$ukipvoter, z = small_dat$income)
res2$par
## smaller than ssr=203
res2$value

plot_res <- ggplot(possible_solutions,aes(x=a,y=b,z=ssr))+
   geom_contour_filled()+
   geom_point(aes(x=res2$par[1],y=res2$par[2]))
print(plot_res)

```


> I also realized that one could calculate summaries by group but the following gives me lots of NAs. I also got confused by ggplot. So many error messages!  Do error messages mean I'm a bad person? I never had error messages in my life until I started using R! I wonder if this means I'm not meant to use R? (I know that this is [fixed mindset](https://www.youtube.com/watch?v=KUWn_TJTrnU) thinking that I should avoid. I'm just a bit stressed about all of the negative feedback I'm getting from R.)

```{r}
bes_fem_group <- bes %>%
  group_by(women_equal) %>%
  summarize(
    prop_ukip_vt = mean(ukipvoter),
    prop_ukip_pid = mean(ukip_pid)
  )

## ukip_fem_plot <- ggplot(data = bes_fem_group,
##         aes(x = women_equal, y = prop_ukip_pid)) +
##     geom_line() + ## using grouped data
##     geom_point() +
##     geom_smooth(data=bes,aes(x=women_equal,y=ukip_pid), ## using orig data
##         method='loess',se=FALSE,span=.8, ## using loess
##         method.args=list(family="symmetric",deg=1))+
##     geom_smooth(data=bes,aes(x=women_equal,y=ukip_pid), ## using orig data
##         method='rlm',se=FALSE)+
##     geom_jitter(data=bes,aes(x=women_equal,y=ukip_pid),width=.1,height=.1)+
##     stat_summary(fun="mean",color="red")
##
## print(ukip_fem_plot)
```

